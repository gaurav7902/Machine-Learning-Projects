{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4852a5d-014f-4e49-a193-7753aea884fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e022b07-1036-466f-a666-df940c927dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate, num_iterations, threshold):\n",
    "        \"\"\"\n",
    "        Initialize Logistic Regression model\n",
    "        \n",
    "        Parameters:\n",
    "        learning_rate (float): Step size for gradient descent\n",
    "        num_iterations (int): Number of training iterations\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        threshold = threshold\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Compute sigmoid function\n",
    "        \n",
    "        Parameters:\n",
    "        z (ndarray): Input values\n",
    "        \n",
    "        Returns:\n",
    "        ndarray: Sigmoid of input values\n",
    "        \"\"\"\n",
    "        # Clip z to avoid overflow\n",
    "        #z = np.clip(z, -500, 500)       lets keep it out first\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def initialize_parameters(self, num_features):\n",
    "        \"\"\"\n",
    "        Initialize weights and bias\n",
    "        \n",
    "        Parameters:\n",
    "        num_features (int): Number of input features\n",
    "        \"\"\"\n",
    "        self.weights = np.ones(num_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "    def compute_cost(self, X, y, y_pred):\n",
    "        \"\"\"\n",
    "        Compute binary cross-entropy loss\n",
    "        \n",
    "        Parameters:\n",
    "        X (ndarray): Feature matrix (m samples × n features)\n",
    "        y (ndarray): True labels\n",
    "        y_pred (ndarray): Predicted probabilities\n",
    "        \n",
    "        Returns:\n",
    "        float: Average loss\n",
    "        \"\"\"\n",
    "        m = len(y)\n",
    "        small_const = 1e-15  # Small constant to avoid log(0)\n",
    "        y_pred = np.clip(y_pred, small_const, 1 - small_const)  # Clip predictions\n",
    "        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using gradient descent\n",
    "        \n",
    "        Parameters:\n",
    "        X (ndarray): Feature matrix (m samples × n features)\n",
    "        y (ndarray): Target labels (0 or 1)\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        self.initialize_parameters(n)\n",
    "        \n",
    "        # Store cost history for debugging\n",
    "        cost_history = []\n",
    "          # Early stopping parameters\n",
    "        epsilon = 1e-3  # Minimum cost change threshold\n",
    "        patience = 10  # Number of iterations to wait for improvement\n",
    "\n",
    "        best_cost = np.inf\n",
    "        no_improvement_count = 0\n",
    "        \n",
    "        for iteration in range(self.num_iterations):\n",
    "            # Forward propagation\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self.sigmoid(z)\n",
    "            \n",
    "            # Compute cost\n",
    "            cost = self.compute_cost(X, y, y_pred)\n",
    "            #cost_history.append(cost)\n",
    "            if cost < best_cost - epsilon:\n",
    "                best_cost = cost\n",
    "                no_improvement_count = 0\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "\n",
    "            if no_improvement_count >= patience:\n",
    "                print(f\"Early stopping at iteration {iteration}\")\n",
    "                break\n",
    "            \n",
    "            # Compute gradients\n",
    "            dz = y_pred - y\n",
    "            dw = (1/m) * np.dot(X.T, dz)\n",
    "            db = (1/m) * np.sum(dz)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Print cost every 100 iterations\n",
    "            # if iteration % 100 == 0:\n",
    "            #     print(f\"Iteration {iteration}: Cost = {cost} and z = {z}\")\n",
    "                \n",
    "        return cost_history\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict probability of class 1\n",
    "        \n",
    "        Parameters:\n",
    "        X (ndarray): Feature matrix\n",
    "        \n",
    "        Returns:\n",
    "        ndarray: Predicted probabilities\n",
    "        \"\"\"\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Predict class labels\n",
    "        \n",
    "        Parameters:\n",
    "        X (ndarray): Feature matrix\n",
    "        threshold (float): Classification threshold\n",
    "        \n",
    "        Returns:\n",
    "        ndarray: Predicted class labels (0 or 1)\n",
    "        \"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        return (probas >= threshold ).astype(int)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate accuracy score\n",
    "        \n",
    "        Parameters:\n",
    "        X (ndarray): Feature matrix\n",
    "        y (ndarray): True labels\n",
    "        \n",
    "        Returns:\n",
    "        float: Accuracy score\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "    def predict_y(self, X_test):\n",
    "        \"\"\"\n",
    "        Predict y values for a new dataset\n",
    "\n",
    "        Parameters:\n",
    "        X_test (ndarray): Feature matrix of the test dataset\n",
    "\n",
    "        Returns:\n",
    "        ndarray: Predicted y values for the test dataset\n",
    "        \"\"\"\n",
    "    def get_weights(self):  # Add self parameter\n",
    "        return np.array(self.weights)\n",
    "\n",
    "    def get_bias(self):  # Add self parameter\n",
    "        return np.array(self.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a08de9-6ef7-4a29-9779-42c34e70f791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaura\\AppData\\Local\\Temp\\ipykernel_14424\\310400503.py:28: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 37\n",
      "\n",
      "Accuracy: 0.9264\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    df = pd.read_csv(\"binary_classification_train.csv\")\n",
    "    \n",
    "    # Assuming the last column is the target (y), and the rest are features (X)\n",
    "    # X = df.iloc[:, 1:-1].values  # All columns except the last one\n",
    "    X = df.iloc[:, 1:-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "\n",
    "    \n",
    "    # Create and train model\n",
    "    #hyperparaneters\n",
    "    threshold = 0.5\n",
    "    learning_rate=0.3\n",
    "    num_iterations=1000\n",
    "    model = LogisticRegression(learning_rate, num_iterations , threshold)\n",
    "    cost_history = model.fit(X, y)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X)\n",
    "    weights = model.get_weights()\n",
    "    bias = model.get_bias()\n",
    "    accuracy = model.score(X, y)\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba3a77ad-20a6-4eca-b1d8-3801cf055484",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Load the test data\n",
    "df1 = pd.read_csv(\"binary_classification_test.csv\")\n",
    "\n",
    "# Assuming all columns except the last are features (X_test)\n",
    "X_test = df1.iloc[:, 1:].values  # Exclude the target column if present\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "340f715d-bb6c-40ba-8e78-88c7422e6ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaura\\AppData\\Local\\Temp\\ipykernel_14424\\310400503.py:28: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities and class labels\n",
    "y_test_pred_proba = model.predict_proba(X_test)  # Predicted probabilities\n",
    "y_test_pred = model.predict(X_test)             # Predicted class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ce82150-07ab-4a3f-a0d1-fd6b40c18659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted probabilities for the test set:\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "\n",
      "Predicted class labels for the test set:\n",
      "[0 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"\\nPredicted probabilities for the test set:\")\n",
    "print(y_test_pred_proba)\n",
    "\n",
    "print(\"\\nPredicted class labels for the test set:\")\n",
    "print(y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be4b97d7-b2b8-450f-be25-bd8867cba6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook binary_classification.ipynb to script\n",
      "[NbConvertApp] Writing 6369 bytes to binary_classification.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script binary_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48afbf36-370e-46bb-b043-8ab3049b3f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
