{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d4852a5d-014f-4e49-a193-7753aea884fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1e022b07-1036-466f-a666-df940c927dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate, num_iterations, threshold):\n",
    "        \"\"\"\n",
    "        Initialize Logistic Regression model\n",
    "        \n",
    "        Parameters:\n",
    "        learning_rate (float): Step size for gradient descent\n",
    "        num_iterations (int): Number of training iterations\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        threshold = threshold\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Compute sigmoid function\n",
    "        \n",
    "        Parameters:\n",
    "        z (ndarray): Input values\n",
    "        \n",
    "        Returns:\n",
    "        ndarray: Sigmoid of input values\n",
    "        \"\"\"\n",
    "        # Clip z to avoid overflow\n",
    "        #z = np.clip(z, -500, 500)       lets keep it out first\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def initialize_parameters(self, num_features):\n",
    "        \"\"\"\n",
    "        Initialize weights and bias\n",
    "        \n",
    "        Parameters:\n",
    "        num_features (int): Number of input features\n",
    "        \"\"\"\n",
    "        self.weights = np.ones(num_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "    def compute_cost(self, X, y, y_pred):\n",
    "        \"\"\"\n",
    "        Compute binary cross-entropy loss\n",
    "        \n",
    "        Parameters:\n",
    "        X (ndarray): Feature matrix (m samples × n features)\n",
    "        y (ndarray): True labels\n",
    "        y_pred (ndarray): Predicted probabilities\n",
    "        \n",
    "        Returns:\n",
    "        float: Average loss\n",
    "        \"\"\"\n",
    "        m = len(y)\n",
    "        small_const = 1e-15  # Small constant to avoid log(0)\n",
    "        y_pred = np.clip(y_pred, small_const, 1 - small_const)  # Clip predictions\n",
    "        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using gradient descent\n",
    "        \n",
    "        Parameters:\n",
    "        X (ndarray): Feature matrix (m samples × n features)\n",
    "        y (ndarray): Target labels (0 or 1)\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        self.initialize_parameters(n)\n",
    "        \n",
    "        # Store cost history for debugging\n",
    "        cost_history = []\n",
    "          # Early stopping parameters\n",
    "        epsilon = 1e-3  # Minimum cost change threshold\n",
    "        patience = 10  # Number of iterations to wait for improvement\n",
    "\n",
    "        best_cost = np.inf\n",
    "        no_improvement_count = 0\n",
    "        \n",
    "        for iteration in range(self.num_iterations):\n",
    "            # Forward propagation\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self.sigmoid(z)\n",
    "            \n",
    "            # Compute cost\n",
    "            cost = self.compute_cost(X, y, y_pred)\n",
    "            #cost_history.append(cost)\n",
    "            if cost < best_cost - epsilon:\n",
    "                best_cost = cost\n",
    "                no_improvement_count = 0\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "\n",
    "            if no_improvement_count >= patience:\n",
    "                print(f\"Early stopping at iteration {iteration}\")\n",
    "                break\n",
    "            \n",
    "            # Compute gradients\n",
    "            dz = y_pred - y\n",
    "            dw = (1/m) * np.dot(X.T, dz)\n",
    "            db = (1/m) * np.sum(dz)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Print cost every 100 iterations\n",
    "            # if iteration % 100 == 0:\n",
    "            #     print(f\"Iteration {iteration}: Cost = {cost} and z = {z}\")\n",
    "                \n",
    "        return cost_history\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict probability of class 1\n",
    "        \n",
    "        Parameters:\n",
    "        X (ndarray): Feature matrix\n",
    "        \n",
    "        Returns:\n",
    "        ndarray: Predicted probabilities\n",
    "        \"\"\"\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Predict class labels\n",
    "        \n",
    "        Parameters:\n",
    "        X (ndarray): Feature matrix\n",
    "        threshold (float): Classification threshold\n",
    "        \n",
    "        Returns:\n",
    "        ndarray: Predicted class labels (0 or 1)\n",
    "        \"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        return (probas >= threshold ).astype(int)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate accuracy score\n",
    "        \n",
    "        Parameters:\n",
    "        X (ndarray): Feature matrix\n",
    "        y (ndarray): True labels\n",
    "        \n",
    "        Returns:\n",
    "        float: Accuracy score\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "    def predict_y(self, X_test):\n",
    "        \"\"\"\n",
    "        Predict y values for a new dataset\n",
    "\n",
    "        Parameters:\n",
    "        X_test (ndarray): Feature matrix of the test dataset\n",
    "\n",
    "        Returns:\n",
    "        ndarray: Predicted y values for the test dataset\n",
    "        \"\"\"\n",
    "    def get_weights(self):  # Add self parameter\n",
    "        return np.array(self.weights)\n",
    "\n",
    "    def get_bias(self):  # Add self parameter\n",
    "        return np.array(self.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "23a08de9-6ef7-4a29-9779-42c34e70f791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaura\\AppData\\Local\\Temp\\ipykernel_36112\\310400503.py:28: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 37\n",
      "\n",
      "Accuracy: 0.9264\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic data\n",
    "    df = pd.read_csv(\"binary_classification_train.csv\")\n",
    "    \n",
    "    # Assuming the last column is the target (y), and the rest are features (X)\n",
    "    # X = df.iloc[:, 1:-1].values  # All columns except the last one\n",
    "    X = df.iloc[:, 1:-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "\n",
    "    \n",
    "    # Create and train model\n",
    "    #hyperparaneters\n",
    "    threshold = 0.5\n",
    "    learning_rate=0.3\n",
    "    num_iterations=1000\n",
    "    model = LogisticRegression(learning_rate, num_iterations , threshold)\n",
    "    cost_history = model.fit(X, y)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X)\n",
    "    weights = model.get_weights()\n",
    "    bias = model.get_bias()\n",
    "    accuracy = model.score(X, y)\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ba3a77ad-20a6-4eca-b1d8-3801cf055484",
   "metadata": {},
   "outputs": [],
   "source": [
    "    df1 = pd.read_csv(\"binary_classification_test.csv\")\n",
    "\n",
    "\n",
    "    # Assuming the last column is the target (y), and the rest are features (X)\n",
    "    # X = df.iloc[:, 1:-1].values  # All columns except the last one\n",
    "    X_test= df1.iloc[:, 1:].values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "eacb6e4a-3f2d-4410-8117-044b41b12a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_reshaped = np.repeat(bias, X_test.shape[1])  # Repeat bias for each element in weights\n",
    "z = np.dot(X_test, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d1bc0820-d372-44de-99f7-4bd331d3304b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 20)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "248e8c30-c17b-4de8-94f8-5e820462d510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "66da3f41-c23e-4131-8ae6-a2ddbe5d45ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "0cba6f70-f860-49e6-8caa-5cf2cfdea313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.29196937e+00,  1.76401862e+00, -2.09784901e+00, -2.96023379e+00,\n",
       "       -2.30530950e+00,  3.15727759e+01,  1.80215753e+00,  2.52035515e+00,\n",
       "        6.30996377e+00,  2.29439420e-02,  1.61275995e-02,  1.39166321e+00,\n",
       "        5.38947690e-01,  5.93123940e+00,  5.48675081e+00, -1.06660500e+01,\n",
       "        1.50216997e+01, -1.41436203e+01, -2.06082187e+00,  4.14285001e+00])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "050c7fbc-9375-4375-b8a4-36a4c9aaa057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4430.68588658, -1098.23096919, -5262.91560224, ...,\n",
       "       -1867.45362315,  3759.09519205, -4999.12374546])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f876ec4-8fbc-491e-b6d1-48f020a2d010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
